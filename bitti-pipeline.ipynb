{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "region = boto3.Session().region_name  # must be in the same region as the S3 data!\n",
    "bucket = \"sagemaker-bitty-magazines\"  # pipeline steps use S3 storage extensively\n",
    "model_package_group_name = f\"BittiModelPackageGroupName\"  # enables model versioning\n",
    "prefix = 'sagemaker_pipelines_bitti'\n",
    "turicreate_logs_path = \"s3://{}/{}/logs\".format(bucket, prefix)\n",
    "\n",
    "print('Bucket: {}'.format(bucket))\n",
    "print('Execution role: {}'.format(role))\n",
    "print('SageMaker ver: ' + sagemaker.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing step\n",
    "\n",
    "Download the dataset, convert it into Turi Create's `SFrame` object, and save the output on S3 in a train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p bitti_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting bitti_source/preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile bitti_source/preprocessing.py\n",
    "\"\"\"Processing step for Turi Create object detection model\n",
    "\n",
    "Adapted from the official Turi Create object detection walkthrough.\n",
    "\"\"\"\n",
    "import csv\n",
    "import logging\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import pip\n",
    "\n",
    "# TODO: add EXIF image rotation in the mix - otherwise other people will\n",
    "#       stub their toes eventually. You don't wanna burn 20 eur training\n",
    "#       the model where the labels don't actually align.\n",
    "\n",
    "# TODO: this, hands-down, wins the hackiness competition - refactor ASAP!\n",
    "try:\n",
    "    import turicreate as tc\n",
    "except ImportError:\n",
    "    pip.main([\"install\", \"turicreate\"])\n",
    "    import turicreate as tc\n",
    "try:\n",
    "    import numpy as np\n",
    "except ImportError:\n",
    "    pip.main([\"install\", \"numpy\"])\n",
    "    import numpy as np\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "def extract_sframes(train_split_fraction, size_x, size_y):\n",
    "    \"\"\"\n",
    "    Takes in images and YOLO-formatted annotations, converts\n",
    "    them into Turi Create's SFrame format, and, finally,\n",
    "    splits and saves training / testing frames as output.\n",
    "\n",
    "    Note that the input/output folders are controlled by the\n",
    "    pipeline definition - they are automatically synced with\n",
    "    their respective S3 destinations.\n",
    "    \"\"\"\n",
    "    base_dir = Path(\"/opt/ml/processing\").resolve()\n",
    "    input_dir = base_dir/\"input\"\n",
    "    output_train_dir = base_dir/\"output_train\"\n",
    "    output_test_dir = base_dir/\"output_test\"\n",
    "\n",
    "    unique_img_names = [f.name.replace(\".jpeg\", \"\")\n",
    "                        for f in input_dir.glob(\"*.jpeg\")]\n",
    "\n",
    "    bbox_data = []\n",
    "    for name in unique_img_names:\n",
    "        image_path = input_dir/f\"{name}.jpeg\"\n",
    "        yolo_annotation_path = input_dir/f\"{name}.txt\"\n",
    "        with open(yolo_annotation_path, newline='\\n') as csvfile:\n",
    "            yoloreader = csv.reader(csvfile, delimiter=',')\n",
    "            for row in yoloreader:\n",
    "                x_cen, y_cen, width, height = [np.float32(v) for v in\n",
    "                                               row[0].split(\" \")[1:]]\n",
    "                bbox_data.append([\n",
    "                    image_path.name,\n",
    "                    \"bitti\",\n",
    "                    int(round(x_cen*size_x)),\n",
    "                    int(round(y_cen*size_y)),\n",
    "                    int(round(width*size_x)),\n",
    "                    int(round(height*size_y))])\n",
    "\n",
    "    # yes, we don't really need an extra csv file here, but the tutorial\n",
    "    # was based around it so I based it around having it... ideally, it\n",
    "    # should be refactored out of the script\n",
    "    tmp_csv_fname = \"/tmp/sframe.csv\"\n",
    "    with open(tmp_csv_fname, \"w+\") as csvfile:\n",
    "        sframe_writer = csv.writer(csvfile, delimiter=',')\n",
    "        sframe_writer.writerow([\"name\", \"label\", \"x\", \"y\", \"width\", \"height\"])\n",
    "        sframe_writer.writerows(bbox_data)\n",
    "\n",
    "    csv_sf = tc.SFrame(tmp_csv_fname)\n",
    "\n",
    "    def row_to_bbox_coordinates(row):  # tutorial artifact\n",
    "        return {'x': row['x'], 'width': row['width'],\n",
    "                'y': row['y'], 'height': row['height']}\n",
    "\n",
    "    csv_sf['coordinates'] = csv_sf.apply(row_to_bbox_coordinates)\n",
    "    # delete no longer needed columns\n",
    "    del csv_sf['x'], csv_sf['y'], csv_sf['width'], csv_sf['height']\n",
    "\n",
    "    sf_images = tc.image_analysis.load_images(str(input_dir),\n",
    "                                              recursive=False,\n",
    "                                              random_order=True)\n",
    "\n",
    "    # Split path to get filename\n",
    "    info = sf_images['path'].apply(lambda path: [Path(path).name])\n",
    "\n",
    "    # Rename columns to 'name'\n",
    "    info = info.unpack().rename({'X.0': 'name'})\n",
    "\n",
    "    # Add to our main SFrame\n",
    "    sf_images = sf_images.add_columns(info)\n",
    "\n",
    "    # Original path no longer needed\n",
    "    del sf_images['path']\n",
    "\n",
    "    # Combine label and coordinates into a bounding box dictionary\n",
    "    csv_sf = csv_sf.pack_columns(['label', 'coordinates'],\n",
    "                                 new_column_name='bbox', dtype=dict)\n",
    "\n",
    "    # Combine bounding boxes of the same 'name' into lists\n",
    "    sf_annotations = csv_sf.groupby('name', {\n",
    "            'annotations': tc.aggregate.CONCAT('bbox')})\n",
    "\n",
    "    sf_all = sf_images.join(sf_annotations, on='name', how='left')\n",
    "    sf_all['annotations'] = sf_all['annotations'].fillna([])\n",
    "\n",
    "    # Make a train-test split\n",
    "    sf_train, sf_test = sf_all.random_split(train_split_fraction)\n",
    "\n",
    "    sf_train.save(str(output_train_dir/'bitti_train.sframe'))\n",
    "    sf_test.save(str(output_test_dir/'bitti_test.sframe'))\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    \"\"\"String-pulling function\n",
    "\n",
    "    Basically, channels the args in the right places and ensures\n",
    "    that the required command-line arguments were passed to it\n",
    "    \"\"\"\n",
    "\n",
    "    # a hacky way to make sure env. variables don't come in empty\n",
    "    if not all([args.image_size_x, args.image_size_y,\n",
    "                args.train_split_fraction]):\n",
    "        raise RuntimeError(\"the following arguments are required: \"\n",
    "                           \"--image-size-x, --image-size-y,\"\n",
    "                           \" --train-split-fraction\")\n",
    "\n",
    "    extract_sframes(train_split_fraction=args.train_split_fraction,\n",
    "                    size_x=args.image_size_x,\n",
    "                    size_y=args.image_size_y)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--image-size-x', type=int,\n",
    "                        required=False, default=4032,\n",
    "                        help='Image size along x-axis (in pixels).')\n",
    "    parser.add_argument('--image-size-y', type=int,\n",
    "                        required=False, default=3024,\n",
    "                        help='Image size along x-axis (in pixels).')\n",
    "    parser.add_argument('--train-split-fraction', type=float,\n",
    "                        required=False, default=0.9,\n",
    "                        help='Training data fraction.')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining some variables\n",
    "\n",
    "s3_input = f\"s3://{bucket}/bitti-data-yolo-format/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import ParameterInteger, ParameterString, ParameterFloat\n",
    "from sagemaker.workflow.steps import CacheConfig\n",
    "\n",
    "cache_config = CacheConfig(enable_caching=True, expire_after=\"30d\")\n",
    "\n",
    "processing_instance_count = ParameterInteger(\n",
    "    name=\"ProcessingInstanceCount\",\n",
    "    default_value=1)\n",
    "\n",
    "processing_instance_type = ParameterString(\n",
    "    name=\"ProcessingInstanceType\",\n",
    "    default_value=\"ml.t3.large\")  # ml.t3.medium runs out of RAM on eval stage\n",
    "\n",
    "training_instance_type = ParameterString(\n",
    "    name=\"TrainingInstanceType\",\n",
    "    default_value=\"ml.c5.9xlarge\")  # TODO: get the GPU tested and running\n",
    "\n",
    "training_batch_size = ParameterInteger(\n",
    "    name=\"TrainingBatchSize\",\n",
    "    default_value=32)\n",
    "\n",
    "training_max_iterations = ParameterInteger(\n",
    "    name=\"MaxIterations\",\n",
    "    default_value=300)\n",
    "\n",
    "training_n_cores = ParameterInteger(\n",
    "    name=\"NumPyLambdaWorkers\",\n",
    "    default_value=36)\n",
    "\n",
    "# this will also be used for preprocessing - TC runs on TF now\n",
    "training_instance_tf_version = ParameterString(\n",
    "    name=\"TrainingInstanceTFVersion\",\n",
    "    default_value=\"2.3\")\n",
    "\n",
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\",\n",
    "    #default_value=\"PendingManualApproval\")\n",
    "    default_value=\"Approved\")\n",
    "\n",
    "model_approval_map_threshold = ParameterFloat(\n",
    "    name=\"ModelApprovalmAPThreshold\",\n",
    "    default_value=0.7)\n",
    "\n",
    "input_data = ParameterString(\n",
    "    name=\"InputData\",\n",
    "    default_value=s3_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "\n",
    "preprocessing_image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"tensorflow\",\n",
    "    image_scope=\"inference\",\n",
    "    region=region,\n",
    "    version=str(training_instance_tf_version),\n",
    "    py_version=\"py37\",\n",
    "    instance_type=processing_instance_type)\n",
    "\n",
    "sframes_preproessor = ScriptProcessor(\n",
    "    image_uri=preprocessing_image_uri,\n",
    "    command=[\"python3\"],\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    env={\"IMAGE_SIZE_X\": \"4032\",  # TODO: make it a pipeline param\n",
    "         \"IMAGE_SIZE_y\": \"3024\",  # TODO: make it a pipeline param\n",
    "         \"TrainSplitFraction\": \"0.9\",  # TODO: make it a pipeline param\n",
    "         },\n",
    "    base_job_name=\"script-sframe-conversion\",\n",
    "    role=role)\n",
    "\n",
    "step_sframe_process = ProcessingStep(\n",
    "    name=\"BittiDataProcessing\",\n",
    "    processor=sframes_preproessor,\n",
    "    inputs=[\n",
    "      ProcessingInput(source=input_data, destination=\"/opt/ml/processing/input\"),  \n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/output_train\"),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/output_test\")\n",
    "    ],\n",
    "    code=\"bitti_source/preprocessing.py\",\n",
    "    cache_config=cache_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now for the main bit - the training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting bitti_source/training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile bitti_source/training.py\n",
    "\"\"\"Training step for Turi Create object detection model\n",
    "\n",
    "Adapted from the official Turi Create object detection walkthrough.\n",
    "\"\"\"\n",
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import pip\n",
    "\n",
    "try:\n",
    "    import turicreate as tc\n",
    "except ImportError:\n",
    "    pip.main([\"install\", \"turicreate\"])\n",
    "    import turicreate as tc\n",
    "try:\n",
    "    import numpy as np\n",
    "except ImportError:\n",
    "    pip.main([\"install\", \"numpy\"])\n",
    "    import numpy as np\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "def train(train_dir, test_dir, output_dir, batch_size, max_iterations, number_pylambda_workers):\n",
    "    train_dir = Path(train_dir).resolve()\n",
    "    test_dir = Path(test_dir).resolve()\n",
    "    output_dir = Path(output_dir).resolve()\n",
    "\n",
    "    logging.info(f\"train_dir is \\\"{train_dir}\\\";test_dir is \\\"{test_dir}\\\"\\n\")\n",
    "    logging.info(f\"train_dir contents are {list(train_dir.glob('*'))}\")\n",
    "    logging.info(f\"test_dir contents are {list(test_dir.glob('*'))}\")\n",
    "\n",
    "    tc.config.set_runtime_config('TURI_DEFAULT_NUM_PYLAMBDA_WORKERS',\n",
    "                                 number_pylambda_workers)\n",
    "\n",
    "    # Load the data\n",
    "    train_data = tc.SFrame(str(train_dir/'bitti_train.sframe'))\n",
    "    test_data = tc.SFrame(str(test_dir/'bitti_test.sframe'))\n",
    "\n",
    "    # Create a model\n",
    "    model = tc.object_detector.create(train_data,\n",
    "                                      max_iterations=max_iterations,\n",
    "                                      batch_size=batch_size)\n",
    "\n",
    "    # Evaluate the model and save the results into a dictionary\n",
    "    metrics = model.evaluate(test_data)\n",
    "    logging.info(metrics)\n",
    "\n",
    "    # Save the model for later use in Turi Create\n",
    "    model.save(str(output_dir/'bitti.model'))\n",
    "\n",
    "    # Export for use in Core ML\n",
    "    model.export_coreml(str(output_dir/'bitti.mlmodel'))\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    \"\"\"String-pulling function\n",
    "\n",
    "    Basically, channels the args in the right places and ensures\n",
    "    that the required command-line arguments were passed to it\n",
    "    \"\"\"\n",
    "\n",
    "    logging.info(\"Recieved the following arguments:\")\n",
    "    logging.info(args)\n",
    "\n",
    "    # a hacky way to make sure env. variables don't come in empty\n",
    "    if not all([args.train, args.test, args.batch_size, args.max_iterations,\n",
    "                args.number_pylambda_workers]):\n",
    "        raise RuntimeError(\"the following arguments are required: \"\n",
    "                           \"--train, --test, --batch-size, --max-iterations,\"\n",
    "                           \" --number-pylabmda-workers\")\n",
    "    train(train_dir=args.train, test_dir=args.test, output_dir=args.model_output,\n",
    "          batch_size=args.batch_size, max_iterations=args.max_iterations,\n",
    "          number_pylambda_workers=args.number_pylambda_workers)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--train', type=str, required=False, default=os.environ.get('SM_CHANNEL_TRAIN'),\n",
    "                        help='The directory where the training data is stored.')\n",
    "    parser.add_argument('--test', type=str, required=False, default=os.environ.get('SM_CHANNEL_TEST'),\n",
    "                        help='The directory where the test input data is stored.')\n",
    "    parser.add_argument('--model-output', type=str, default=os.environ.get('SM_MODEL_DIR'),\n",
    "                        help='The directory where the trained model will be stored.')\n",
    "    parser.add_argument('--batch-size', type=int,\n",
    "                        required=False, default=32)\n",
    "    parser.add_argument('--max-iterations', type=int,\n",
    "                        required=False, default=300)\n",
    "    parser.add_argument('--number-pylambda-workers', type=int,\n",
    "                        required=False, default=36)  # TODO: read this from SM_NUM_CPUS!\n",
    "    parser.add_argument('--model_dir', type=str,\n",
    "                        help=\"This is the S3 URI for model's file storage and so on.\"\n",
    "                             \" It appears it always gets passed via SageMaker TrainingStep.\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "\n",
    "model_path = f\"s3://{bucket}/output_model\"\n",
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"tensorflow\",\n",
    "    image_scope=\"training\",\n",
    "    region=region,\n",
    "    version=str(training_instance_tf_version),\n",
    "    py_version=\"py37\",\n",
    "    instance_type=training_instance_type)\n",
    "\n",
    "# Regular expressions are a pain, use the playground here: https://regex101.com/r/kopij0/1\n",
    "# TODO: works for reporting, but how to publish it via CloudWatch metrics as well?\n",
    "turicreate_metrics = [{'Name': 'train:loss', 'Regex': r\"(?:\\| [0-9]+ \\| )([0-9]+[.][0-9]+)\"}]\n",
    "\n",
    "tf_train = TensorFlow(base_job_name='bitti-turicreate-pipelines',\n",
    "                      entry_point='training.py',\n",
    "                      source_dir='bitti_source',\n",
    "                      output_path=model_path,  # don't use model_dir hyperparam!\n",
    "                      role=role,\n",
    "                      image_uri=image_uri,\n",
    "                      hyperparameters={'max-iterations': int(training_max_iterations),\n",
    "                                       'batch-size': int(training_batch_size),\n",
    "                                       'number-pylambda-workers': int(training_n_cores)\n",
    "                                      },\n",
    "                      instance_count=1,\n",
    "                      instance_type=str(training_instance_type),\n",
    "                      metric_definitions=turicreate_metrics,\n",
    "                      input_mode='File')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "\n",
    "# TODO: change into Pipe - but that would need additional read f-ions in training.py\n",
    "\n",
    "step_train = TrainingStep(\n",
    "    name=\"ModelTraining\",\n",
    "    estimator=tf_train,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(step_sframe_process.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "                               #content_type=\"application/octet-stream\",  # Tested, not needed for File mode\n",
    "                               input_mode=\"File\"),\n",
    "        \"test\": TrainingInput(step_sframe_process.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri,\n",
    "                              #content_type=\"application/octet-stream\",\n",
    "                              input_mode=\"File\")},\n",
    "    cache_config=cache_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the evaluation step\n",
    "\n",
    "We have trained the model, but we need to validate it too. What happened the first time was, due to the EXIF tags half of images were not rotated properly relative to the labels. Needless to say, that led to the mAP score to be close to nill. That's a good example for why the validation step is needed - only models that *work* should be carried on with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting bitti_source/evaluation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile bitti_source/evaluation.py\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import tarfile\n",
    "import pip\n",
    "try:\n",
    "    import turicreate as tc\n",
    "except ImportError:\n",
    "    pip.main([\"install\", \"turicreate\"])\n",
    "    import turicreate as tc\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # define the folder structure we inherited from SageMaker pipelines\n",
    "    model_dir = Path(\"/opt/ml/processing/model\").resolve()\n",
    "    test_dir = Path(\"/opt/ml/processing/test\").resolve()\n",
    "    output_dir = Path(\"/opt/ml/processing/evaluation\").resolve()\n",
    "\n",
    "    # load and untar the model file\n",
    "    model_path = model_dir/\"model.tar.gz\"\n",
    "    with tarfile.open(model_path) as tar:\n",
    "        tar.extractall(path=\".\")\n",
    "    model = tc.load_model(\"bitti.model\")\n",
    "    logging.info(\"Loaded the model from %s\", str(model_path))\n",
    "\n",
    "    # load and score the testing data\n",
    "    test_path = test_dir/\"bitti_test.sframe\"\n",
    "    test_data = tc.SFrame(str(test_path))  # don't pass raw Path instances\n",
    "    metrics = model.evaluate(test_data)\n",
    "    logging.info(\"Evaluating the model on test data: %s\", metrics)\n",
    "    mAP = metrics['mean_average_precision_50']\n",
    "\n",
    "    report_dict = {\"regression_metrics\": {\"mAP\": {\"value\": mAP}}}\n",
    "\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    evaluation_path = output_dir/\"evaluation.json\"\n",
    "\n",
    "    with open(evaluation_path, \"w\") as f:\n",
    "        f.write(json.dumps(report_dict))\n",
    "    if evaluation_path.exists():\n",
    "        logging.info(\"Successfully dumped evaluation JSON to `%s`.\",\n",
    "                     str(evaluation_path))\n",
    "    else:\n",
    "        logging.error(\"Failed writing the evaluation file!\")\n",
    "    logging.info(\"Evaluation script finished. Storing mAP=%.2f into the evaluation report\", mAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "\n",
    "\n",
    "script_eval = ScriptProcessor(\n",
    "    image_uri=image_uri,\n",
    "    command=[\"python3\"],\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=1,\n",
    "    base_job_name=\"script-bitti-eval\",\n",
    "    role=role)\n",
    "\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"EvaluationReport\",\n",
    "    output_name=\"evaluation\",\n",
    "    path=\"evaluation.json\")\n",
    "\n",
    "step_eval = ProcessingStep(\n",
    "    name=\"ModelEvaluation\",\n",
    "    processor=script_eval,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination=\"/opt/ml/processing/model\"),\n",
    "        ProcessingInput(\n",
    "            source=step_sframe_process.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/test\")],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\")],\n",
    "    code=\"bitti_source/evaluation.py\",\n",
    "    property_files=[evaluation_report])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep, JsonGet\n",
    "\n",
    "\n",
    "cond_map = ConditionGreaterThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step=step_eval,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"regression_metrics.mAP.value\"),\n",
    "    right=model_approval_map_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics \n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "\n",
    "\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=\"{}/evaluation.json\".format(\n",
    "            step_eval.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "        ),\n",
    "        content_type=\"application/json\"\n",
    "    )\n",
    ")\n",
    "step_register = RegisterModel(\n",
    "    name=\"BittiRegisterModel\",\n",
    "    estimator=tf_train,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"application/octet-stream\"],\n",
    "    response_types=[\"application/octet-stream\"],\n",
    "    inference_instances=[\"ml.t2.medium\", \"ml.m5.xlarge\"],\n",
    "    transform_instances=[\"ml.m5.xlarge\"],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status=model_approval_status,\n",
    "    model_metrics=model_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting bitti_source/publish_to_api.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile bitti_source/publish_to_api.py\n",
    "import time\n",
    "import tarfile\n",
    "import boto3\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # load the model\n",
    "    model_path = \"/opt/ml/processing/model/model.tar.gz\"\n",
    "    with tarfile.open(model_path) as tar:\n",
    "        tar.extractall(path=\".\")\n",
    "    model_file = \"bitti.mlmodel\"\n",
    "\n",
    "    mlfilename=f'bitti-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())}.mlmodel'\n",
    "\n",
    "    bucket_name = \"magazine-monitor\"\n",
    "    s3 = boto3.resource('s3')\n",
    "    s3.Bucket(bucket_name).upload_file(model_file, f\"Models/{mlfilename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_publish = ScriptProcessor(\n",
    "    image_uri=image_uri,\n",
    "    command=[\"python3\"],\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=1,\n",
    "    base_job_name=\"script-bitti-publish\",\n",
    "    role=role)\n",
    "\n",
    "step_publish = ProcessingStep(\n",
    "    name=\"PublishViaAPI\",\n",
    "    processor=script_publish,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination=\"/opt/ml/processing/model\")],\n",
    "    code=\"bitti_source/publish_to_api.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_cond = ConditionStep(\n",
    "    name=\"BittymAPcheck\",\n",
    "    conditions=[cond_map],\n",
    "    if_steps=[step_register, step_publish],\n",
    "    else_steps=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "\n",
    "pipeline_name = \"BittiPipeline\"\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_count,\n",
    "        processing_instance_type,\n",
    "        training_instance_type,\n",
    "        training_batch_size,\n",
    "        training_max_iterations,\n",
    "        training_n_cores,\n",
    "        training_instance_tf_version,\n",
    "        model_approval_status,\n",
    "        input_data,\n",
    "        model_approval_map_threshold\n",
    "    ],\n",
    "    steps=[step_sframe_process, step_train, step_eval, step_cond],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test run of the pipeline - we probably don't want to burn resources\n",
    "# just to see that there was a typo at the end of the training script\n",
    "\n",
    "# FIXME: doesn't work!\n",
    "\n",
    "#execution = pipeline.start(\n",
    "#    parameters=dict(\n",
    "#        TrainingInstanceType=\"ml.m5.large\",\n",
    "#        NumPyLambdaWorkers=1,\n",
    "#        MaxIterations=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.describe()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
