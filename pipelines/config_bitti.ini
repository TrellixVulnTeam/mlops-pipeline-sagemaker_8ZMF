[secretsmanager]
# pre-made secrets manager keys to avoid leaking stuff on github
secret_role = bitti-pipeline-execution-role
secret_account_id = bitti-pipeline-account-id

[metadata]
pipeline_name = BittiPipeline
# path to pipeline .py module relative to origin of this config file
source_dir = step_scripts
# sic: should be "bitti" in an ideal world
bucket = sagemaker-bitty-magazines
# region be the same for both the bucket and ECR images
region = eu-central-1
# passing this along enables model versioning
model_package_group_name = BittiModelPackageGroupName
cache_steps = true
cache_expire_after = 30d

[training]
# uses horovod for TF, but I haven't tested it for non-unity values
instance_count = 1
# the right instance_type for the right batch_size is not 100% optimized
instance_type = ml.g4dn.4xlarge
image_uri_fmt = {}.dkr.ecr.${metadata:region}.amazonaws.com/tf-turicreate:latest
# turicreate freaks out and then reinstalls other TF versions
# currently the value below is not used - I've built a manual docker training image
# leaving the "{}" in to format with `account_id` after a call to secrets manager
#tf_version = 2.0.4
# note: leaving batch_size or max_iterations empty will let turicreate set
# values to (educated) defaults; in this case, we don't fully agree with
# max_iterations and want to change it - the dataset was heavily augmented
batch_size = 32
max_iterations = 18000
# TODO: add metrics name and regex below

[processing]
# for simplicity's sake, all processing steps are assumed to run with the same
# parameters; but, depending on your use case, it may be smarter to split into
# multiple config blocks describing types, counts, ECR URIs for different steps
instance_count = 1
# scaled it because ml.t3.medium runs out of RAM on eval stage
instance_type = ml.m5.large
image_uri_fmt = {}.dkr.ecr.${metadata:region}.amazonaws.com/tf-turicreate:processing
train_test_split_fraction = 0.9
# this is being fed to the first pipeline step as input data;
# the follow-up inputs/outputs are defined via sagemaker sdk
input_data = s3://${metadata:bucket}/bitti-data-yolo-format/
# TODO: add data augmentation parameters here

[evaluation]
# mAP cut above which the model gets approved with ${approval_status}
approval_map_threshold = 0.7
approval_status = Approved
